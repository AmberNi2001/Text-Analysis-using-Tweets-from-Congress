---
title: "Texts Analysis using Tweets from Congress"
author: "Amber"
date: "2025-02-03"
output:
  html_document: default
  pdf_document: default
---

# Project Description

This project analyzes tweets from politicians, focused on pre-processing and analyzing textual contents. The project employs various natural language processing techniques - follows a systematic workflow, including:

1. Document sampling
2. Text preprocessing (tokenization, cleaning, and stemming)
3. Exploratory analysis using word clouds and TF-IDF to visualize word frequencies andhighlight discriminative terms
5. Sentiment classification through VADER dictionary to categorize tweets as positive, neutral, or negative
6. Similarity analysis through cosine similarity to measure thematic consistency

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)
rm(list=ls())
setwd("~/Desktop/Github_Projects/Text-Analysis-using-Tweets-from-Congress") 
getwd()

# Install pacman if not already installed
# install.packages("pacman")

# Install necessary packages
pacman::p_load(tidyverse, ggplot2, quanteda, tidytext) 
```

```{r load}
tweets <- read_csv("~/Desktop/Github_Projects/tweets_congress.csv")
```

### Part 1

I **took a sample** of my documents and read them carefully. This sample is not random; instead, I chose documents that are particularly interesting to me (**tweets from only democrats**). After reviewing the sample, I reflect on my observations, key takeaways, and any notable patterns or insights that emerge.

#### Filter, subset and read my documents
```{r}
# Filter all tweets from democrats
tweets_dem <- tweets %>%
  filter(Party == "D")
head(tweets_dem)

# Dataset too large ; create a subset.
set.seed(1) # Ensure we get the same output of a randomization
tweets_dem_sub <- tweets_dem %>%
  sample_n(50000) # Create a subset of 50000 observations

# Randomly read 10 rows
set.seed(2)
tweets_dem_sub %>%
  sample_n(10) 
```
From the **data** side, these documents are long and messy, containing texts and author information. These texts include a lot of emojis, mentions , hash tags, and URLs which can make pre-processing complicated. These also include HTML escape characters like &amp which should be "&" and line breaks (\n\n).

From the **content** side, after reading the tweets from some of the democrats, I found out that they brought up a lot frequently mentioned issues that democrats care about, including "LGBTQIA+", "BorderWall","newest citizen","Covid-19", etc. These relate to political topics like immigration policy, government spending, and civic engagement. We can also see some strong sentiments behind these tweets by looking at words like "useless" and "congrats".

### Part 2

I **tokenized** my documents and **pre-processed** them by removing any extraneous content identified during my close reading of a sample. I evaluated which content to remove based on its relevance and impact on the analysis. 

#### Create a corpus
```{r}
# Convert the dataset into a corpus 
tweets_corpus <- corpus(tweets_dem_sub, text_field = "text")
summary(tweets_corpus, n =5) # Look at the first 5 documents
```

#### Tokenization and Pre-processing
```{r}
# Tokenize the corpus 

text_clean <- gsub("'s|'m|'re|'d|'ve|'ll|n't|’s|’m|’re|’d|’ve|’ll|n’t", "", tweets_corpus)  # Remove common English contractions and avoid empty spaces being recognized by a token

dem_tokens <- tokens(text_clean, remove_punct = TRUE, remove_numbers = TRUE) %>% # Remove punctuation and numbers 
  tokens_tolower() %>% # Convert all tokens to lower case
  tokens_remove(c(stopwords("en"), "madam", "mr", "today","rt")) %>% # Remove common English stopwords, retweet identifier and frequently appeared meaningless words
  tokens_remove(pattern = "#*|@*") %>%  # Remove hash tags and handles
  tokens_remove(pattern = "^https://.*", valuetype = "regex") %>% # Remove web links
  tokens_remove(c("amp","$")) %>% # Remove HTML escape characters
  tokens_wordstem() # Word stemming 

# Look at the first few tokenized texts
head(dem_tokens)
```

During my tokenization and preprocessing, I first cleaned the texts in my corpus by **removing common English contractions**. Then, I tokenized by **removing punctuation and numbers** and converting all tokens to **lowercase**. I removed **common English stop words** like "and" and meaningless words like "mr" that often appear in political contents. I also removed **hash tags (#), mentions (@) , re-tweets and website links** starting with "https". After I glimpsed the tokens and found out that there are a lot of words "amp" which can be resulted from **HTML escape characters** like &amp, I removed all "amp". Besides, I used **word stemming** to reduce duplicated words and reduce the dataset size for faster analysis later on.

I **didn't use n-grams** because I don't think it makes sense to make all individual tokens into, say, bi-grams as some words might make sense but most of them won't. 

### Part 3

I identified **location** as an important source of variation in my data. I then **subseted** the data along Northeastern and Western states and **created word clouds** for each category. Finally, I examined the differences between the word clouds to identify notable patterns or variations.

#### Subset my data along location, tokenize and preprocess 
```{r}
# Subset my original dataset by states that are in the West 
tweets_west <- tweets_dem_sub %>%
  filter(State == "CA" | State == "WA" | State == "OR" | State == "AK" | State == "HI"
         | State == "MT" | State == "ID" | State == "WY" | State == "NV" | State == "UT" 
         | State == "CO" | State == "AZ" | State == "NM")

# Subset my original dataset by states that are in the Northeast 
tweets_east <- tweets_dem_sub %>%
  filter(State == "ME" | State == "NH" | State == "VT" | State == "MA" | State == "RI"
         | State == "CT" | State == "NY" | State == "NJ" | State == "PA")

# Create corpus for both 
west_corpus <- corpus(tweets_west, text_field = "text")
east_corpus <- corpus(tweets_east, text_field = "text")

# Tokenize and pre-process both corpuses 

## West 

text_clean_west <- gsub("'s|'m|'re|'d|'ve|'ll|n't|’s|’m|’re|’d|’ve|’ll|n’t", "", west_corpus) # Remove common English contractions and avoid empty spaces being recognized by a token

west_tokens <- tokens(text_clean_west, remove_punct = TRUE, remove_numbers = TRUE) %>% # Remove punctuation and numbers 
  tokens_tolower() %>% # Convert all tokens to lower case
  tokens_remove(c(stopwords("en"), "madam", "mr", "today","rt")) %>% # Remove common English stopwords, retweets identifier and frequetly appeared meaningless words
  tokens_remove(pattern = "#*|@*") %>%  # Remove hashtags and handles
  tokens_remove(pattern = "^https://.*", valuetype = "regex") %>% # Remove web links
  tokens_remove(c("amp","$")) %>% # Remove HTML escape characters
  tokens_wordstem() # Word stemming 

## Northeast

text_clean_east <- gsub("'s|'m|'re|'d|'ve|'ll|n't|’s|’m|’re|’d|’ve|’ll|n’t", "", east_corpus) # Remove common English contractions and avoid empty spaces being recognized by a token

east_tokens <- tokens(text_clean_east, remove_punct = TRUE, remove_numbers = TRUE) %>% # Remove punctuation and numbers 
  tokens_tolower() %>% # Convert all tokens to lower case
  tokens_remove(c(stopwords("en"), "madam", "mr", "today","rt")) %>% # Remove common English stopwords, retweets identifier and frequetly appeared meaningless words
  tokens_remove(pattern = "#*|@*") %>%  # Remove hashtags and handles
  tokens_remove(pattern = "^https://.*", valuetype = "regex") %>% # Remove web links
  tokens_remove(c("amp","$")) %>% # Remove HTML escape characters
  tokens_wordstem() # Word stemming 

# Convert tokens to Document Term Matrix(DFM) and trim
west_dfm <- west_tokens %>% 
  dfm() %>%
  dfm_trim(min_docfreq = 0.001, max_docfreq = 0.999, docfreq_type = "prop", verbose = TRUE) # useing 0.001 as a document frequency threshold (a frequently used threshold) is too strict

east_dfm <- east_tokens %>% 
  dfm() %>%
  dfm_trim(min_docfreq = 0.001, max_docfreq = 0.999, docfreq_type = "prop", verbose = TRUE)

# Check top features of each DFM
topfeatures(west_dfm)
topfeatures(east_dfm)

```

#### Create word clouds for comparison
```{r}
# Load package necessary to create word clouds
pacman::p_load(quanteda.textplots)

# Create a word cloud for democrats' tweets from the West 

wcWest <- textplot_wordcloud(west_dfm, random_order = FALSE,
                   rotation = .25, max_words = 100,
                   min_size = 0.5, max_size = 2.8,
                   color = RColorBrewer::brewer.pal(8, "Set3"))

# Create a word cloud for democrats' tweets from the Northeast 

wcEast <- textplot_wordcloud(east_dfm, random_order = FALSE,
                   rotation = .25, max_words = 100,
                   min_size = 0.5, max_size = 2.8,
                   color = RColorBrewer::brewer.pal(8, "Set3"))
```

From these **word clouds**, there are generally **no major difference** between the contents between these two regions. They all mentioned "American" and "work" the most often, followed by words like "work", "family" , "community" and "trump".

### Part 4

I subseted my data along the same variation and **identified words that differentiate between groups**. To achieve this, I applied **TF-IDF (Term Frequency-Inverse Document Frequency)**. This method **highlighted words that are more distinctive** in one group compared to others. By analyzing these key terms, I gained insights into **how language use varies across different categories** and what words are **most representative** of each group.

#### Compute TF-IDF for my data for both groups 
```{r}
pacman::p_load(quanteda.textstats)

# Proportional TF and log-scaled IDF for West
tfidf_west <- west_dfm %>%
  dfm_tfidf(scheme_tf = "prop", scheme_df = "inversemax")  

# Proportional TF and log-scaled IDF for Northeast
tfidf_east <- east_dfm %>%
  dfm_tfidf(scheme_tf = "prop", scheme_df = "inversemax")  

# Get top words for states in the West
top_west <- textstat_frequency(tfidf_west, n = 10, force = TRUE)  # Top 10 words
top_west <- top_west %>% select(feature, frequency) # Keeps only the word (feature) and its TF-IDF frequency

# Get top words for states in the Northeast
top_east <- textstat_frequency(tfidf_east, n = 10, force = TRUE) 
top_east <- top_east %>% select(feature, frequency)

# This gives us top words that are most distinctive for each group based on TF-IDF values.

print(top_west)
print(top_east)
```

Several words appear in both lists with high frequency, suggesting shared themes across both datasets. Words like **“happi” (happy), “celebr” (celebrate), and “honor”** indicate a strong presence of **positive emotions**. Additionally, **“discuss”, “read”, and “hear”** suggest that **engagement with content** — whether through conversation, reading, or listening—is a common theme in both groups.

### Part 5

I **utilized an existing dictionary called VADER** to measure sentiment, tone. I then **labeled** my documents based on the sentiment categories and **visualized** the prevalence of each class to analyze trends and patterns in the data.

```{r}
# Here I used VADER as it is useful for sentiment analysis on social media contents like tweets.
library(vader)

# Write a function to tidy the results
get_vader_tidy <- function(text_clean){
  get_vader(text_clean) %>% 
    tibble(outcomes=names(.), 
           values=.)
}  

# Apply VADER to the first 100 comments and store results in a tidy format
vader_outputs <- map(tweets_dem_sub$text[1:100], get_vader_tidy)

# Bind all outputs into a single data frame and merge with original dataset
dem_vader <- tweets_dem_sub %>%
  slice(1:100) %>%   # Keep only the first 1000 rows (matching `vader_outputs`)
  mutate(vader_output = vader_outputs) %>%   # Attach sentiment results
  unnest(vader_output) 

# Filter for compound sentiment scores and select relevant columns (text, outcomes and values)
dem_vader_fil <- dem_vader %>%
  filter(outcomes == "compound") %>%
  select(text, outcomes, values)
head(dem_vader_fil)

# Sort from highest to lowest sentiment score
sorted_sentiment <- dem_vader_fil %>%
  arrange(desc(values)) 
head(sorted_sentiment)

# Label my documents
sorted_sentiment <- sorted_sentiment %>%
  mutate(sentiment = case_when(
    values > 0  ~ "positive",
    values < 0  ~ "negative",
    values == 0 ~ "neutral"
  ))

# Merge labels to my original dataset
merged_dem_vader <- left_join(dem_vader_fil, sorted_sentiment, by = "text") 
head(merged_dem_vader)

# Count sentiment classes
sent_counts <- sorted_sentiment %>%
  count(sentiment)

# Create a bar plot to visualize the appearance of class
ggplot(sent_counts, aes(x = sentiment, y = n, fill = sentiment)) +
  geom_col() +
  labs(title = "Sentiment Class Distribution across Tweets from Democrats", x = "Sentiment Class", y = "Text Count") +
  scale_y_continuous(breaks = seq(0, 60, by = 5)) +
  theme_minimal()
```

The visualization reveals that **the majority of tweets exhibit a positive sentiment** (depicted in blue), significantly outnumbering both neutral (green) and negative (red) tweets. The counts for neutral and negative sentiments are relatively close, each accounting for roughly one-third of the positive sentiment count. This suggests that **Democratic tweets tend to convey more optimistic messages**.

### Part 6

I seleced at least 10 documents that clearly, with high probability, represent each class in my sentiment classification dictionary.

#### Part 6.2

I used **cosine similarity** to identify the **10 documents** that are **most similar** to **a reference document with highest sentiment score** within each class. After reviewing these documents, I evaluated how well cosine similarity captures meaningful similarities between texts. Additionally, I experimented with **Euclidean distance measure** to compare the results and assess whether it provides different or improved document retrieval. This comparison helped determine **the effectiveness of different similarity measures** in identifying related content.

```{r}
# Arrange with descending compound score
merged_dem_vader <- merged_dem_vader %>%
  arrange(desc(values.y))

# Convert to matrix 
vader_dfm_matrix <- merged_dem_vader %>%
  corpus() %>%
  tokens() %>%
  dfm() %>%
  as.matrix()

# Extract vectors for reference documents 
mostpos <- vader_dfm_matrix["text1", ] # most positive
mostneg <- vader_dfm_matrix["text100", ] # most negative

# Function to calculate cosine similarity
calculate_cosine_similarity <- function(vec1, vec2) {
  dot_product <- sum(vec1 * vec2)
  magn1 <- sqrt(sum(vec1^2))
  magn2 <- sqrt(sum(vec2^2))
  return(dot_product / (magn1 * magn2)) 
}

# [Cosine similarity] Distance and similarity with reference to the most positive one
cosine_scores_pos <- apply(vader_dfm_matrix, 1, function(text) calculate_cosine_similarity(text, mostpos))
cosine_results_pos <- data.frame(text = rownames(vader_dfm_matrix), cosine_similarity = cosine_scores_pos)

# Identify the 10 closet documents to the most positive 

top10pos_co <- cosine_results_pos %>%
  arrange(desc(cosine_similarity)) %>%  # Sort in descending order
  slice(1:10)
print(top10pos_co)

# [Cosine similarity] Distance and similarity with reference to the most negative one
cosine_scores_neg <- apply(vader_dfm_matrix, 1, function(text) calculate_cosine_similarity(text, mostneg))
cosine_results_neg <- data.frame(text = rownames(vader_dfm_matrix), cosine_similarity = cosine_scores_neg)

# Identify the 10 closet documents to the most negative

top10neg_co <- cosine_results_neg %>%
  arrange(desc(cosine_similarity)) %>%  # Sort in descending order
  slice(1:10)
print(top10neg_co)
```

**Cosine Similarity** focuses on the direction of text vectors rather than their magnitude, making it useful for comparing documents of different lengths.

```{r}
# Function to calculate Euclidean distance between two vectors
calculate_euclidean_distance <- function(vec1, vec2) {
  ec_distance <- sqrt(sum((vec1 - vec2)^2)) # Euclidean formula 
  return(ec_distance)
}

# [Euclidean similarity] Distance and similarity with reference to the most positive one
eu_scores_pos <- apply(vader_dfm_matrix, 1, function(text) calculate_euclidean_distance(text, mostpos))
eu_results_pos <- data.frame(text = rownames(vader_dfm_matrix), euclidean_similarity = eu_scores_pos)

# Identify the 10 closet documents to the most positive 

top10pos_eu <- eu_results_pos %>%
  arrange(desc(euclidean_similarity)) %>%  # Sort in descending order
  slice(1:10)
print(top10pos_eu)

# [Euclidean similarity] Distance and similarity with reference to the most negative one
eu_scores_neg<- apply(vader_dfm_matrix, 1, function(text) calculate_euclidean_distance(text, mostneg))
eu_results_neg <- data.frame(text = rownames(vader_dfm_matrix), euclidean_similarity = eu_scores_neg)

# Identify the 10 closet documents to the most negative

top10neg_eu <- eu_results_neg %>%
  arrange(desc(euclidean_similarity)) %>%  # Sort in descending order
  slice(1:10)
print(top10neg_eu)

# Combine cosine and Euclidean similarity results for positive reference
top10pos_combined <- bind_rows(
  top10pos_co %>% mutate(similarity_type = "cosine"),
  top10pos_eu %>% mutate(similarity_type = "euclidean")
) 
print(top10pos_combined)

# Combine cosine and Euclidean similarity results for negative reference
top10neg_combined <- bind_rows(
  top10neg_co %>% mutate(similarity_type = "cosine"),
  top10neg_eu %>% mutate(similarity_type = "euclidean")
) 
print(top10neg_combined)
```

Using **Euclidean similarity** makes the results totally different, as it measures straight-line distance between two points in space. It focuses on the absolute size of text content in a document.

#### Q 6.3

I qualitatively analyzed the retrieved documents by examining their **top features** or **highest TF-IDF terms** to understand their most representative words. Additionally, I used **keyword-in-context (KWIC) analysis** to see how these terms appear within the text. Based on this qualitative assessment, I evaluated whether my sentiment dictionary accurately captures the intended themes or if adjustments are necessary. 

```{r}
# Select the top 10 texts based on the highest cosine similarity scores
top10_texts_pos <- cosine_results_pos %>%
  arrange(desc(cosine_similarity)) %>% # Sort the results in descending order of cosine similarity
  slice(1:10) %>% # Select the top 10 entries
  pull(text)  # Extract text IDs

# Convert the merged_dem_vader dataframe into a corpus for text analysis
dem_vader_corpus <- merged_dem_vader %>%
  corpus()

# Subset the corpus to include only the documents that match the top 10 (most positive) text IDs
subset_corpus_vadar <- corpus_subset(dem_vader_corpus, docnames(dem_vader_corpus) %in% top10_texts_pos)

# Create a Document-Feature Matrix (DFM) and apply TF-IDF weighting
tfdif_vader_pos <- dfm(tokens(subset_corpus_vadar)) %>%
  dfm_tfidf(scheme_tf = "prop", scheme_df = "inversemax")

# Convert the TF-IDF weighted DFM into a data frame
tfidf_df_pos <- convert(tfdif_vader_pos, to = "data.frame")

# Reshape and sort the top terms by TF-IDF score
top_tfidf_terms <- tfidf_df_pos  %>%
  pivot_longer(-doc_id, names_to = "term", values_to = "tfidf") %>%
  arrange(desc(tfidf)) %>%
  slice(1:10)  # Select the 10 highest TF-IDF terms
print(top_tfidf_terms)

# Define key words to search for in the text
patterns <- c("worth","one","serve","from","wage")

# Tokenize the first 10 texts from the merged_dem_vader dataframe
vader_tokens_toppos <- merged_dem_vader %>%
  slice(1:10) %>%
  corpus(text_field = "text") %>%
  tokens() 

# Perform keyword-in-context (KWIC) search for the defined patterns within a window of 5 words
kwic_results_pos <- kwic(vader_tokens_toppos, pattern = patterns, window = 5)
print(kwic_results_pos)
```
I think sentiment analysis here using Vader is a bit weird, as I looked at those key words and found out they are actually unrelated to sentiment. Those are more thematic words based on context, since, for example, serve shows strong thematic relevance like public service or civic duty. Therefore, if I would chnage my dictionary, I will change a topic dictionary.